{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": [],
   "private_outputs": true,
   "authorship_tag": "ABX9TyPwMiHzSdjUaz8vkLwSlXRl",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hungkien05/notebook/blob/master/my_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvY0N78yh5un",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "mt5BXm3MvKKE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    #/content/drive/MyDrive/Data/\n",
    "    lines = open('train.%s' % (lang1), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # tien xu ly: https://analyticsindiamag.com/how-to-create-a-vocabulary-builder-for-nlp-tasks/\n",
    "    \n",
    "\n",
    "    return lines"
   ],
   "metadata": {
    "id": "8EwPljepvORi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.functional import Tensor\n",
    "class Vocab:\n",
    "  def __init__(self, lang, lines):\n",
    "    self.lang=lang\n",
    "    self.lines=lines\n",
    "    self.words = set()\n",
    "    #self.indices = []\n",
    "    self.dictionary = dict()\n",
    "    self.word_to_idx= dict()\n",
    "    self.indices = torch.LongTensor()\n",
    "    self.size=0\n",
    "\n",
    "  def add_sentence(self,sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.words.add(word)\n",
    "\n",
    "  def build_vocab(self):\n",
    "    for sentence in lines:\n",
    "      self.add_sentence(sentence)\n",
    "    self.word_to_idx = {word: i for i,word in enumerate(self.words)}\n",
    "    self.indices = torch.LongTensor([self.word_to_idx[word] for word in self.words]) # create index tensor\n",
    "    self.size = len(self.words)\n",
    "\n",
    "    \n"
   ],
   "metadata": {
    "id": "TDR2RTGNHjMC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lines = readLangs(\"en\", \"vi\")\n",
    "eng_vocab = Vocab(\"en\", lines)\n",
    "eng_vocab.build_vocab()\n",
    "#eng_vocab.words\n"
   ],
   "metadata": {
    "id": "YKCpYzChZA3h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4974, -0.5363,  0.5842,  ..., -0.9518,  0.4767,  0.0030],\n",
      "        [ 0.5484,  1.1578,  1.5077,  ..., -0.7099, -1.3904,  0.4672],\n",
      "        [-0.7445,  0.1795,  0.6405,  ..., -0.0036,  0.0749, -2.1796],\n",
      "        ...,\n",
      "        [-1.0967,  0.3138,  0.3261,  ..., -0.0932, -0.3927,  0.5050],\n",
      "        [-0.0393, -0.0626, -0.1809,  ..., -1.2488,  1.0228, -0.7464],\n",
      "        [-1.0946, -0.5065, -0.9193,  ..., -0.6371,  0.6114, -0.1085]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embed = nn.Embedding(eng_vocab.size, 10)\n",
    "print(embed(eng_vocab.indices))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layer=1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, enc_input):\n",
    "        tmp = self.embed(enc_input)\n",
    "        h0 = torch.randn(self.n_layer, self.batch_size, self.hidden_dim)\n",
    "        c0 = torch.randn(self.n_layer, self.batch_size, self.hidden_dim)\n",
    "        encoder_output, (hn,cn)= self.lstm(tmp, h0, c0)\n",
    "        return (hn,cn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim= input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, dec_input):\n",
    "        h0 = torch.randn(self.n_layer, self.batch_size, self.hidden_dim)\n",
    "        c0 = torch.randn(self.n_layer, self.batch_size, self.hidden_dim)\n",
    "        dec_output, (hn,cn)= self.lstm(dec_input, h0, c0)\n",
    "        return dec_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}